{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from json import loads\n",
    "import pandas as pd\n",
    "import psycopg2 \n",
    "import csv\n",
    "from urllib import parse, request\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract the features from DB\n",
    "conn = psycopg2.connect(\"dbname='SQLBook' user='postgres' host='localhost' password='postgres' password='1234'\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "#categories: dataframe that contains categories information from Asterixdb (nodeId and 5 levels of categories)\n",
    "#categories.to_csv('categories.csv')\n",
    "\n",
    "#reviewFeatures: dataframe that contains review features from Postgres\n",
    "#reviewFeatures.to_csv('reviewFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to get review length\n",
    "def rev_len(x):\n",
    "    #print (x)\n",
    "    x['reviewlen']=len(str(x['txt']))\n",
    "    return x\n",
    "\n",
    "#Function to get review length\n",
    "def rev_len(x):\n",
    "    #print (x)\n",
    "    x['reviewlen']=len(str(x['txt']))\n",
    "    return x\n",
    "\n",
    "#Get number of sentences\n",
    "def count_sentences(x):\n",
    "    cnt=0\n",
    "    num_exc=0\n",
    "    for i in str(x):\n",
    "        if i in ['.','?','!']:\n",
    "            cnt=cnt+1\n",
    "    for i in str(x):\n",
    "        if i in ['?','!']:\n",
    "            num_exc=num_exc+1\n",
    "    return cnt,num_exc \n",
    "\n",
    "#Get average word length\n",
    "def avg_word_len(x):\n",
    "    word_list = str(x).split(' ')\n",
    "    nwords = len(word_list)\n",
    "    tot_len=0\n",
    "    for w in word_list:\n",
    "        tot_len=tot_len+len(w)\n",
    "    return tot_len/nwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT r.reviewID,r.asin, cast(r.overall as int) as rating, cast(trim(leading '[' from substring(helpful from 0 for position(',' in helpful))) as int) as votesForReview,cast(trim(trailing ']' from substring(helpful from position(',' in helpful)+2)) as int) as outOf,reviewCount.cnt as reviewerCount,bookCount.cnt as bookreviewCount,EXTRACT(day from current_date - r.reviewTime) age,pr.nodeID,reviewtext,summary FROM reviews r LEFT OUTER JOIN (SELECT reviewID, count(*) cnt FROM reviews GROUP BY reviewID) reviewCount ON r.reviewID = reviewCount.reviewID LEFT OUTER JOIN (SELECT asin, count(*) cnt FROM reviews GROUP BY asin) bookCount ON r.asin = bookCount.asin LEFT OUTER JOIN Products pr ON r.asin = pr.asin\")\n",
    "rows = cur.fetchall()\n",
    "\n",
    "df = pd.DataFrame(rows,columns=['reviewID','asin','rating','helpfulness','outOf','reviewerCount','bokReviewCount','reviewAge','nodeID','reviewtext','summary'])\n",
    "\n",
    "#Combining summary and review text\n",
    "df['txt']=df['reviewtext']+df['summary']\n",
    "df.drop(['reviewtext','summary'],axis=1,inplace=True)\n",
    "\n",
    "#Get review length\n",
    "df['reviewlen']=0\n",
    "df=df.apply(lambda x: rev_len(x),axis=1)\n",
    "\n",
    "#Get # of words, word length, # of sentences, ARI index\n",
    "df['numwords']=0\n",
    "df['avgwordlen']=0\n",
    "df['num_sen']=0\n",
    "df['num_exc']=0\n",
    "df['ARI']=0\n",
    "\n",
    "def numwords(x):\n",
    "    x['numwords']=len(str(x['txt']).split(' '))\n",
    "    x['avgwordlen'] = avg_word_len(x['txt'])\n",
    "    x['num_sen'],x['num_exc'] = count_sentences(x['txt'])\n",
    "    x['ARI'] = 4.71*(len(str(x['txt']))/float(x['numwords']+1)) + 0.5*(x['numwords']/float(x['num_sen']+1)) - 21.43\n",
    "    return x\n",
    "df=df.apply(lambda x: numwords(x),axis=1)\n",
    "\n",
    "#Extract just the feature set from original df\n",
    "new_df = df[['rating','helpfulness','outOf','reviewerCount','bokReviewCount','reviewAge','reviewlen','numwords','avgwordlen','num_sen','num_exc','ARI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alison/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Training data is the reviews with more than 1 outof\n",
    "train_df=new_df[new_df['outOf']!=0]\n",
    "\n",
    "#Prediction data is the reviews which dont have any outof\n",
    "test_df=new_df[new_df['outOf']==0]\n",
    "\n",
    "#Get the training labels\n",
    "y_train = train_df['helpfulness']/train_df['outOf']\n",
    "\n",
    "#Drop the label data from train\n",
    "train_df.drop(['helpfulness','outOf'],axis=1,inplace=True)\n",
    "\n",
    "#Split the data into train, validate, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df, y_train, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X_train_n = scaler.transform(X_train)\n",
    "X_test_n = scaler.transform(X_test)\n",
    "X_val_n = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit linear regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_n,y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the output for train and validate\n",
    "y_train_pred = reg.predict(X_train_n)\n",
    "y_val_pred=reg.predict(X_val_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13904183387410762"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the error for train and validate\n",
    "mean_squared_error(y_train.values,y_train_pred)\n",
    "mean_squared_error(y_val.values,y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13847058382099395"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict for test data\n",
    "y_test_pred=reg.predict(X_test_n)\n",
    "\n",
    "#Check the error\n",
    "mean_squared_error(y_test.values,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alison/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/Alison/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Actual prediction on unknown labels\n",
    "test_df.drop(['helpfulness','outOf'],axis=1,inplace=True)\n",
    "\n",
    "test_df_n = scaler.transform(test_df) \n",
    "\n",
    "y_test_df=reg.predict(test_df_n)\n",
    "\n",
    "#Store the redicted help ratio\n",
    "test_df['help_ratio']=y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alison/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#Create help ratio in original df\n",
    "new_df['help_ratio']=new_df['helpfulness']/new_df['outOf']\n",
    "\n",
    "# Join \n",
    "new_df=new_df.join(test_df['help_ratio'],how='left',rsuffix='1')\n",
    "\n",
    "#Get the final ratio\n",
    "def final_ratio(x):\n",
    "    if np.isnan(x['help_ratio']):\n",
    "        x['help_ratio']=x['help_ratio1']\n",
    "    return x\n",
    "\n",
    "new_df=new_df.apply(lambda x: final_ratio(x),axis=1)\n",
    "new_df.drop('help_ratio1',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.join(new_df['help_ratio'],how='left')\n",
    "df['rating*help_ratio']=df['rating']*df['help_ratio']\n",
    "df_grouped = df.groupby('asin').sum()\n",
    "\n",
    "\n",
    "def get_count(x):\n",
    "    x['count']=len(df[df['asin']==x['asin']])\n",
    "    return x\n",
    "\n",
    "df_grouped['count']=0\n",
    "df_grouped=df_grouped.reset_index().apply(lambda x: get_count(x),axis=1)\n",
    "\n",
    "df_grouped['overall_rating1'] = (df_grouped['rating*help_ratio']/df_grouped['count']) + np.log(df_grouped['count'])\n",
    "#df_grouped=df_grouped.sort_values(by='overall_rating1',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get categories mapping, index to category name\n",
    "cats_map=[]\n",
    "with open('numcats.txt','r') as f:\n",
    "    reader=csv.reader(f,delimiter='\\t')\n",
    "    for num,cat in reader:\n",
    "        cats_map.append(cat)\n",
    "        \n",
    "#Get books mapping, index to asin\n",
    "asin = np.load('asin.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asterixdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QueryResponse:\n",
    "    def __init__(self, raw_response):\n",
    "        self._json = loads(raw_response)\n",
    "\n",
    "        self.requestID = self._json['requestID'] if 'requestID' in self._json else None\n",
    "        self.clientContextID = self._json['clientContextID'] if 'clientContextID' in self._json else None\n",
    "        self.signature = self._json['signature'] if 'signature' in self._json else None\n",
    "        self.results = self._json['results'] if 'results' in self. _json else None\n",
    "        self.metrics = self._json['metrics'] if 'metrics' in self._json else None\n",
    "\n",
    "class AsterixConnection:\n",
    "    def __init__(self, server = 'http://localhost', port = 19002):\n",
    "        self._server = server\n",
    "        self._port = port\n",
    "        self._url_base = self._server +':'+ str(port)\n",
    "\n",
    "    def query(self, statement, pretty=False, client_context_id=None):\n",
    "        endpoint = '/query/service'\n",
    "\n",
    "        url = self._url_base + endpoint\n",
    "\n",
    "        payload = {\n",
    "            'statement': statement,\n",
    "            'pretty': pretty\n",
    "        }\n",
    "\n",
    "        if client_context_id:\n",
    "            payload['client_context_id'] = client_context_id\n",
    "\n",
    "        data = parse.urlencode(payload).encode(\"utf-8\")\n",
    "        req = request.Request(url, data)\n",
    "        response = request.urlopen(req).read()\n",
    "\n",
    "        return QueryResponse(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get category information, grouped by nodeID\n",
    "asterix_conn = AsterixConnection()\n",
    "response = asterix_conn.query(\"\"\"USE TinySocial;\n",
    "    SELECT \n",
    "        nodeID, \n",
    "        (SELECT gp.cat.category.nested.level_1, gp.cat.category.nested.nested.level_2, gp.cat.category.nested.nested.nested.level_3, gp.cat.category.nested.nested.nested.nested.level_4, gp.cat.category.nested.nested.nested.nested.nested.level_5 FROM gp) AS categories\n",
    "    FROM ClassificationInfo c\n",
    "    GROUP BY c.nodeID AS nodeID\n",
    "    GROUP AS gp(c as cat)\"\"\")\n",
    "# \"\"\"\n",
    "\n",
    "#Parse result from asterixdb and create a dictionary {nodeID: [[lvl1,lvl2,lvl3,lvl4,lvl5],[lvl1,lvl2,..],..]}\n",
    "max_cat = 0\n",
    "categories = []\n",
    "\n",
    "for i in range(len(response.results)):\n",
    "    if len(response.results[i]['categories']) > max_cat:\n",
    "        max_cat = len(response.results[i]['categories'])\n",
    "    c = []\n",
    "    for x in range(len(response.results[i]['categories'])):\n",
    "        c.append([cats_map.index(response.results[i]['categories'][x][l]) for l in response.results[i]['categories'][x]])\n",
    "    categories.append([response.results[i]['nodeID'],c])\n",
    "    \n",
    "categories = {i[0]:i[1] + [[0,0,0,0,0]]*(max_cat-len(i[1])) for i in categories}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get nodeid to asin mapping\n",
    "cur.execute(\"\"\"select nodeid,asin from products\"\"\")\n",
    "rows = cur.fetchall()\n",
    "nodeid_asin = pd.DataFrame(rows,columns=['nodeid','asin'])\n",
    "nodeid_asin = nodeid_asin.set_index('asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reorder categories with the nodeid to asin mapping (from postgres) and index to asin mapping (from asin.npy)\n",
    "categories_indexed = []\n",
    "\n",
    "for i in range(max_cat):\n",
    "    c = []\n",
    "    for a in asin:\n",
    "        c.append(categories[int(nodeid_asin.ix[a.decode()]['nodeid'])][i])\n",
    "    categories_indexed.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get seasonal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"select asin,\n",
    "          round(100*sum(case when month >= 3 and month < 6 then numunits else 0 end)/sum(numunits),2) as spring,\n",
    "          round(100*sum(case when month >= 6 and month < 9 then numunits else 0 end)/sum(numunits),2) as summer,\n",
    "          round(100*sum(case when month >= 9 and month < 12 then numunits else 0 end)/sum(numunits),2) as fall,\n",
    "          round(100*sum(case when (month = 12 or month < 3) then numunits else 0 end)/sum(numunits),2) as winter,\n",
    "          cast(max(fullprice) as decimal) fullprice,\n",
    "          case when max(isinstock) = 'Y' then 1 else 0 end isinstock\n",
    "          from \n",
    "          (select asin, EXTRACT(MONTH FROM orderdate) as month, case when l.numunits = 0 then 0.00001 else l.numunits end as numunits, fullprice, isinstock\n",
    "          from customers c, orders o, orderlines l, products p\n",
    "          where c.customerid = o.customerid\n",
    "          and o.orderid = l.orderid\n",
    "          and l.productid = p.productid\n",
    "          ) as temp\n",
    "          group by asin\"\"\")\n",
    "# \"\"\"\n",
    "rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "season = pd.DataFrame(rows,columns=['asin','spring','summer','fall','winter','fullprice','isinstock'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asin_final = []\n",
    "\n",
    "for i in asin:\n",
    "    if i.decode() in df_grouped.asin.values:\n",
    "        row = [i for i in df_grouped[df_grouped.asin==i.decode()][['overall_rating1','count']].values[0]]\n",
    "    else:\n",
    "        row = [0,0]\n",
    "        \n",
    "    if i.decode() in season.asin.values:\n",
    "        row += [i for i in season[season.asin==i.decode()][['spring','summer','fall','winter','fullprice','isinstock']].values[0]]\n",
    "    else:\n",
    "        row += [0,0,0,0,0,'']\n",
    "        \n",
    "    asin_final.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.1680567331125546,\n",
       "  3.0,\n",
       "  Decimal('0.00'),\n",
       "  Decimal('33.33'),\n",
       "  Decimal('0.00'),\n",
       "  Decimal('66.67'),\n",
       "  Decimal('540.00'),\n",
       "  1],\n",
       " [2.0769230769230766,\n",
       "  1.0,\n",
       "  Decimal('0.00'),\n",
       "  Decimal('33.33'),\n",
       "  Decimal('33.33'),\n",
       "  Decimal('33.33'),\n",
       "  Decimal('306.00'),\n",
       "  1],\n",
       " [4.8026425700974524,\n",
       "  3.0,\n",
       "  Decimal('25.00'),\n",
       "  Decimal('0.00'),\n",
       "  Decimal('25.00'),\n",
       "  Decimal('50.00'),\n",
       "  Decimal('340.00'),\n",
       "  1],\n",
       " [5.6732632446133442,\n",
       "  28.0,\n",
       "  Decimal('0.00'),\n",
       "  Decimal('100.00'),\n",
       "  Decimal('0.00'),\n",
       "  Decimal('0.00'),\n",
       "  Decimal('375.00'),\n",
       "  1],\n",
       " [2.2219158793519567,\n",
       "  2.0,\n",
       "  Decimal('0.00'),\n",
       "  Decimal('100.00'),\n",
       "  Decimal('0.00'),\n",
       "  Decimal('0.00'),\n",
       "  Decimal('540.00'),\n",
       "  1]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#overall_rating, count, spring, summer, fall, winter, price, instock\n",
    "asin_final[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rating_indexed = np.array(asin_final)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_indexed[rating_indexed==0] = np.min(rating_indexed[rating_indexed != 0])/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"rating_indexed.npy\",rating_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('season_price_instock_indexed.npy',np.array(asin_final, dtype=np.float)[:,2:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
